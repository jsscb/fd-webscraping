{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7205c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "from pathlib import Path\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import NoSuchElementException, ElementClickInterceptedException\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf28451a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup headless Chrome\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")  # Remove this line if you want to see browser\n",
    "options.add_argument(\"--disable-gpu\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01011647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔗 Found 7 products with >1000 reviews.\n"
     ]
    }
   ],
   "source": [
    "# ========== STEP 1: Load all product links ==========\n",
    "\n",
    "brand_url = \"https://reviews.femaledaily.com/brands/product/cosrx\"\n",
    "driver.get(brand_url)\n",
    "time.sleep(3)\n",
    "\n",
    "# Click \"Load More\" until all products are shown\n",
    "while True:\n",
    "    try:\n",
    "        load_more_button = driver.find_element(By.ID, \"button-load-more-products\")\n",
    "        driver.execute_script(\"arguments[0].click();\", load_more_button)\n",
    "        time.sleep(2)\n",
    "    except NoSuchElementException:\n",
    "        break\n",
    "    except ElementClickInterceptedException:\n",
    "        time.sleep(2)\n",
    "        continue\n",
    "\n",
    "# Parse loaded product page\n",
    "soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "product_cards = soup.find_all(\"a\", class_=\"product-card\")\n",
    "product_links = []\n",
    "\n",
    "# Filter only products with >300 reviews\n",
    "for card in product_cards:\n",
    "    rating_span = card.find(\"span\", class_=lambda x: x and \"fd-body-sm-regular\" in x and \"grey\" in x)\n",
    "    if rating_span:\n",
    "        review_text = rating_span.get_text()\n",
    "        match = re.search(r\"\\(([\\d,]+)\\)\", review_text)\n",
    "        if match:\n",
    "            review_count = int(match.group(1).replace(\",\", \"\"))\n",
    "            if review_count > 1000:\n",
    "                link = card.get(\"href\")\n",
    "                if link and link.startswith(\"/\"):\n",
    "                    link = \"https://reviews.femaledaily.com\" + link\n",
    "                product_links.append(link)\n",
    "\n",
    "print(f\"🔗 Found {len(product_links)} products with >1000 reviews.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c9f684c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📄 Scraping: https://reviews.femaledaily.com/products/cleanser/toner/cosrx/aha-bha-clarifying-treatment-toner\n",
      "🔄 Loading page 452 for AHA/BHA Clarifying Treatment Toner\n",
      "⛔ No more reviews at page 452\n",
      "\n",
      "📄 Scraping: https://reviews.femaledaily.com/products/cleanser/facial-wash/cosrx/good-morning-gel-cleanser\n",
      "🔄 Loading page 442 for Low pH Good Morning Gel Cleanser\n",
      "⛔ No more reviews at page 442\n",
      "\n",
      "📄 Scraping: https://reviews.femaledaily.com/products/treatment/serum-essence/cosrx/x-advanced-snail-96-mucin-power-essence\n",
      "🔄 Loading page 376 for Advanced Snail 96 Mucin Power Essence\n",
      "⛔ No more reviews at page 376\n",
      "\n",
      "📄 Scraping: https://reviews.femaledaily.com/products/treatment/acne-treatment/cosrx/acne-pimple-master-patch-1\n",
      "🔄 Loading page 142 for Acne Pimple Master Patch\n",
      "⛔ No more reviews at page 142\n",
      "\n",
      "📄 Scraping: https://reviews.femaledaily.com/products/moisturizer/lotion-emulsion/cosrx/oil-free-ultra-moisturizing-lotion-with-birch-sap\n",
      "🔄 Loading page 137 for Oil-free Ultra-Moisturizing Lotion (with Birch Sap)\n",
      "⛔ No more reviews at page 137\n",
      "\n",
      "📄 Scraping: https://reviews.femaledaily.com/products/cleanser/scrub-exfoliator/cosrx/one-step-pimple-clear-set-pad\n",
      "🔄 Loading page 38 for One Step Original Clear Pad\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'all_reviews_checkpoint.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 106\u001b[0m\n\u001b[0;32m     93\u001b[0m     all_reviews\u001b[38;5;241m.\u001b[39mappend({\n\u001b[0;32m     94\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProduct Name\u001b[39m\u001b[38;5;124m\"\u001b[39m: product_name,\n\u001b[0;32m     95\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReview Date\u001b[39m\u001b[38;5;124m\"\u001b[39m: review_date,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    102\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReview Page\u001b[39m\u001b[38;5;124m\"\u001b[39m: page\n\u001b[0;32m    103\u001b[0m     })\n\u001b[0;32m    105\u001b[0m \u001b[38;5;66;03m# Save after each page\u001b[39;00m\n\u001b[1;32m--> 106\u001b[0m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_reviews\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Page \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m done for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mproduct_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Total: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(all_reviews)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    108\u001b[0m page \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Tasya\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\generic.py:3902\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3891\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[0;32m   3893\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[0;32m   3894\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[0;32m   3895\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3899\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[0;32m   3900\u001b[0m )\n\u001b[1;32m-> 3902\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3903\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3904\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3905\u001b[0m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3906\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3907\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3908\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3909\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquoting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3910\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3911\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3912\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3913\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3914\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3916\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoublequote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mescapechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3918\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3919\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Tasya\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\formats\\format.py:1152\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m   1131\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1133\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[0;32m   1134\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[0;32m   1135\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[0;32m   1151\u001b[0m )\n\u001b[1;32m-> 1152\u001b[0m \u001b[43mcsv_formatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[0;32m   1155\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[1;32mc:\\Users\\Tasya\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\formats\\csvs.py:247\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    244\u001b[0m \u001b[38;5;124;03mCreate the writer & save.\u001b[39;00m\n\u001b[0;32m    245\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[1;32m--> 247\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    248\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    249\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    250\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[0;32m    255\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[0;32m    256\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[0;32m    257\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[0;32m    258\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    263\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[0;32m    264\u001b[0m     )\n\u001b[0;32m    266\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save()\n",
      "File \u001b[1;32mc:\\Users\\Tasya\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    865\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    867\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    868\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'all_reviews_checkpoint.csv'"
     ]
    }
   ],
   "source": [
    "# ========== STEP 2: Scrape Reviews ==========\n",
    "\n",
    "# Checkpoint save file\n",
    "checkpoint_file = \"all_reviews_checkpoint.csv\"\n",
    "checkpoint_path = Path(checkpoint_file)\n",
    "\n",
    "# Load previous progress if exists\n",
    "if checkpoint_path.exists():\n",
    "    checkpoint_df = pd.read_csv(checkpoint_file)\n",
    "else:\n",
    "    checkpoint_df = pd.DataFrame(columns=[\n",
    "        \"Product Name\", \"Review Date\", \"Review Rating\", \"Recommend\", \"Review Text\",\n",
    "        \"Usage Period\", \"Purchase Point\", \"Product URL\", \"Review Page\"\n",
    "    ])\n",
    "\n",
    "# Function to load page with retry\n",
    "def safe_get(url, retries=3, backoff=2):\n",
    "    for i in range(retries):\n",
    "        try:\n",
    "            driver.set_page_load_timeout(180)\n",
    "            driver.get(url)\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error loading {url}: {e}\")\n",
    "            time.sleep(backoff * (2 ** i) + random.uniform(0, 1))\n",
    "    return False\n",
    "\n",
    "# Helper to get last scraped page\n",
    "def get_last_scraped_page(url):\n",
    "    df = checkpoint_df[checkpoint_df[\"Product URL\"] == url]\n",
    "    return df[\"Review Page\"].max() if not df.empty else 0\n",
    "\n",
    "all_reviews = checkpoint_df.to_dict(\"records\")\n",
    "\n",
    "for product_url in product_links:\n",
    "    print(f\"\\n📄 Scraping: {product_url}\")\n",
    "    if not safe_get(product_url):\n",
    "        print(f\"❌ Skipping {product_url} due to repeated timeouts.\")\n",
    "        continue\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    try:\n",
    "        product_name = soup.find(\"h1\").get_text(strip=True)\n",
    "    except:\n",
    "        product_name = \"N/A\"\n",
    "\n",
    "    page = get_last_scraped_page(product_url) + 1\n",
    "\n",
    "    while True:\n",
    "        review_url = f\"{product_url.split('?')[0]}?page={page}\"\n",
    "        print(f\"🔄 Loading page {page} for {product_name}\")\n",
    "\n",
    "        if not safe_get(review_url):\n",
    "            print(f\"⚠️ Timeout at page {page}, moving to next product\")\n",
    "            break\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        reviews = soup.find_all(\"div\", class_=\"review-content-wrapper\")\n",
    "        if not reviews:\n",
    "            print(f\"⛔ No more reviews at page {page}\")\n",
    "            break\n",
    "\n",
    "        for review in reviews:\n",
    "            try:\n",
    "                review_date = review.find(\"p\", class_=\"review-date\").get_text(strip=True)\n",
    "            except:\n",
    "                review_date = \"N/A\"\n",
    "\n",
    "            try:\n",
    "                stars = review.find(\"div\", class_=\"review-card-rating-wrapper\").find_all(\"i\", class_=\"icon-ic_big_star_full\")\n",
    "                review_rating = len(stars)\n",
    "            except:\n",
    "                review_rating = \"N/A\"\n",
    "\n",
    "            try:\n",
    "                recommend = review.find(\"p\", class_=\"recommend\").get_text(strip=True)\n",
    "            except:\n",
    "                recommend = \"N/A\"\n",
    "\n",
    "            try:\n",
    "                review_text = review.find(\"p\", class_=\"text-content\").get_text(strip=True)\n",
    "            except:\n",
    "                review_text = \"N/A\"\n",
    "\n",
    "            try:\n",
    "                info = review.find(\"div\", class_=\"information-wrapper\").find_all(\"p\")\n",
    "                usage_period = info[0].find(\"b\").get_text(strip=True)\n",
    "                purchase_point = info[1].find(\"b\").get_text(strip=True)\n",
    "            except:\n",
    "                usage_period = \"N/A\"\n",
    "                purchase_point = \"N/A\"\n",
    "\n",
    "            all_reviews.append({\n",
    "                \"Product Name\": product_name,\n",
    "                \"Review Date\": review_date,\n",
    "                \"Review Rating\": review_rating,\n",
    "                \"Recommend\": recommend,\n",
    "                \"Review Text\": review_text,\n",
    "                \"Usage Period\": usage_period,\n",
    "                \"Purchase Point\": purchase_point,\n",
    "                \"Product URL\": product_url,\n",
    "                \"Review Page\": page\n",
    "            })\n",
    "\n",
    "        # Save after each page\n",
    "        pd.DataFrame(all_reviews).to_csv(checkpoint_file, index=False)\n",
    "        print(f\"✅ Page {page} done for {product_name}. Total: {len(all_reviews)}\")\n",
    "        page += 1\n",
    "\n",
    "driver.quit()\n",
    "print(f\"\\n🎉 ALL DONE! Scraped {len(all_reviews)} reviews in total.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
